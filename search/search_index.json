{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes-Playground Handbook The Kubernetes-Playground is a solution to setup a simple Kubernetes-Cluster with minimal effort, to learn and play with Kubernetes, see GitHub Project layout . # Project root \u251c\u2500\u2500 .github # the github workflow files \u251c\u2500\u2500 bin # project specific binaries, like plantuml.jar; excluded from version control \u251c\u2500\u2500 docs # documentation \u251c\u2500\u2500 kubernetes-setup # Ansible scripts to provision the vagrant box \u251c\u2500\u2500 mkdocs.yml # configuration file for mkdocs \u251c\u2500\u2500 README.md # Readme file for github \u2514\u2500\u2500 Vagrantfile # Configuration of the vagrant box","title":"Home"},{"location":"#kubernetes-playground-handbook","text":"The Kubernetes-Playground is a solution to setup a simple Kubernetes-Cluster with minimal effort, to learn and play with Kubernetes, see GitHub","title":"Kubernetes-Playground Handbook"},{"location":"#project-layout","text":". # Project root \u251c\u2500\u2500 .github # the github workflow files \u251c\u2500\u2500 bin # project specific binaries, like plantuml.jar; excluded from version control \u251c\u2500\u2500 docs # documentation \u251c\u2500\u2500 kubernetes-setup # Ansible scripts to provision the vagrant box \u251c\u2500\u2500 mkdocs.yml # configuration file for mkdocs \u251c\u2500\u2500 README.md # Readme file for github \u2514\u2500\u2500 Vagrantfile # Configuration of the vagrant box","title":"Project layout"},{"location":"documentation-guide/","text":"Documentation Guide Mkdocs and PlantUML is used to generate the Handbook of this project. It is helpful to install mkdocs to generate a searchable html version of this handbook. Mkdocs Commands # Create a new project. mkdocs new [dir-name] # Start the live-reloading docs server. mkdocs serve # Build the documentation site. mkdocs build # Print this help message. mkdocs help PlantUML PlantUML is a tool to create diagrams from plain text files must be placed in the folder docs/diagrams as a suffix for PlantUML files, .plantuml is used this is required for the Makefile, to create image on the local machine and the GitHub Action to build and deploy the GitHub Page of this project The Visual Studio Code plugin, provides code highlighting, code completion and a live preview, ... Example Diagram Makefile The Makefile to create png or svg images from plain text is in the docs folder it is just needed for local development In the case that plantuml is not in the bin folder of this project, it will be automatically downloaded Overview Targets Target Description all Build png and svg images png Build png images svg Build svg images clean Delete all png and svg images URLs https://plantuml.com/ https://marketplace.visualstudio.com/items?itemName=jebbs.plantuml","title":"Documentation"},{"location":"documentation-guide/#documentation-guide","text":"Mkdocs and PlantUML is used to generate the Handbook of this project. It is helpful to install mkdocs to generate a searchable html version of this handbook.","title":"Documentation Guide"},{"location":"documentation-guide/#mkdocs-commands","text":"# Create a new project. mkdocs new [dir-name] # Start the live-reloading docs server. mkdocs serve # Build the documentation site. mkdocs build # Print this help message. mkdocs help","title":"Mkdocs Commands"},{"location":"documentation-guide/#plantuml","text":"PlantUML is a tool to create diagrams from plain text files must be placed in the folder docs/diagrams as a suffix for PlantUML files, .plantuml is used this is required for the Makefile, to create image on the local machine and the GitHub Action to build and deploy the GitHub Page of this project The Visual Studio Code plugin, provides code highlighting, code completion and a live preview, ...","title":"PlantUML"},{"location":"documentation-guide/#example-diagram","text":"","title":"Example Diagram"},{"location":"documentation-guide/#makefile","text":"The Makefile to create png or svg images from plain text is in the docs folder it is just needed for local development In the case that plantuml is not in the bin folder of this project, it will be automatically downloaded","title":"Makefile"},{"location":"documentation-guide/#overview-targets","text":"Target Description all Build png and svg images png Build png images svg Build svg images clean Delete all png and svg images","title":"Overview Targets"},{"location":"documentation-guide/#urls","text":"https://plantuml.com/ https://marketplace.visualstudio.com/items?itemName=jebbs.plantuml","title":"URLs"},{"location":"installation/","text":"Installation Guide The Kubernetes-Playground is build and tested on Ubuntu 20.04. Hence, the described steps are focusing on Ubuntu. It should be possible to use any Operation system as a host. Required Before starting to use this project, make sure that the following software is installed on the system: VirtualBox, https://www.virtualbox.org/wiki/Downloads Vagrant, https://www.vagrantup.com/downloads apt install virtualbox vagrant Optional - Documentation To generate a searchable HTML documentation mkdocs must be installed on the system # Install mkdocs sudo apt install mkdocs # Install pip sudo apt install python3-pip # Make sure latest version is used pip3 install --upgrade mkdocs Plantuml By the first use of he makefile in the docs folder, plantuml is downloaded For some diagrams plantuml depends on graphviz. Hence, it must be installed. # Install graphviz sudo apt install graphviz","title":"Installation"},{"location":"installation/#installation-guide","text":"The Kubernetes-Playground is build and tested on Ubuntu 20.04. Hence, the described steps are focusing on Ubuntu. It should be possible to use any Operation system as a host.","title":"Installation Guide"},{"location":"installation/#required","text":"Before starting to use this project, make sure that the following software is installed on the system: VirtualBox, https://www.virtualbox.org/wiki/Downloads Vagrant, https://www.vagrantup.com/downloads apt install virtualbox vagrant","title":"Required"},{"location":"installation/#optional-documentation","text":"To generate a searchable HTML documentation mkdocs must be installed on the system # Install mkdocs sudo apt install mkdocs # Install pip sudo apt install python3-pip # Make sure latest version is used pip3 install --upgrade mkdocs","title":"Optional - Documentation"},{"location":"installation/#plantuml","text":"By the first use of he makefile in the docs folder, plantuml is downloaded For some diagrams plantuml depends on graphviz. Hence, it must be installed. # Install graphviz sudo apt install graphviz","title":"Plantuml"},{"location":"kubectl-config/","text":"Configuration of kubectl The next bash snippets outlines, the installation and configuration to use kubectl to access microk8s in a vagrant box. Install kubectl # Install kubectl via snap sudo snap install kubectl --classic kubectl configuration # Create the configuration folder in the home path mkdir .kube # get the configuration from the vagrant box # if the config file already exist, it is needed to merge them vagrant ssh -c microk8s.config >> ~/.kube/config # change the server ip to the external ip of the vagrant box or k8s.home vi ~/.kube/config Instead of the external IP Address of the vagrant box it is also possible to use the domain k8s.home . Hence, the local DNS must resolve the domain to the IP of the vagrant box. This can be done by modifying the /etc/hosts file or the configuration of the local DNS resolver like Unbound. Configuration via update script The updateKubeConfig.sh provides the following functionality Copy the configuration from the vagrant box, to the current directory Replace the IP with the domain k8s.home set the environment variable KUBECONFIG to the file from the first step the first dot is important; otherwise the environment variable is not set from the current shell, only this configuration file is usable the configuration applies only temporarily to the open shell session . ./updateKubeConfig.sh bash bash-completion sudo kubectl completion bash >/etc/bash_completion.d/kubectl","title":"Kubectl"},{"location":"kubectl-config/#configuration-of-kubectl","text":"The next bash snippets outlines, the installation and configuration to use kubectl to access microk8s in a vagrant box.","title":"Configuration of kubectl"},{"location":"kubectl-config/#install-kubectl","text":"# Install kubectl via snap sudo snap install kubectl --classic","title":"Install kubectl"},{"location":"kubectl-config/#kubectl-configuration","text":"# Create the configuration folder in the home path mkdir .kube # get the configuration from the vagrant box # if the config file already exist, it is needed to merge them vagrant ssh -c microk8s.config >> ~/.kube/config # change the server ip to the external ip of the vagrant box or k8s.home vi ~/.kube/config Instead of the external IP Address of the vagrant box it is also possible to use the domain k8s.home . Hence, the local DNS must resolve the domain to the IP of the vagrant box. This can be done by modifying the /etc/hosts file or the configuration of the local DNS resolver like Unbound.","title":"kubectl configuration"},{"location":"kubectl-config/#configuration-via-update-script","text":"The updateKubeConfig.sh provides the following functionality Copy the configuration from the vagrant box, to the current directory Replace the IP with the domain k8s.home set the environment variable KUBECONFIG to the file from the first step the first dot is important; otherwise the environment variable is not set from the current shell, only this configuration file is usable the configuration applies only temporarily to the open shell session . ./updateKubeConfig.sh","title":"Configuration via update script"},{"location":"kubectl-config/#bash-bash-completion","text":"sudo kubectl completion bash >/etc/bash_completion.d/kubectl","title":"bash bash-completion"},{"location":"network-configuration/","text":"Network Configuration Sorry, this is an open todo","title":"Network Configuration"},{"location":"network-configuration/#network-configuration","text":"Sorry, this is an open todo","title":"Network Configuration"},{"location":"run-container-for-analysis/","text":"Run a Container for Analysis It can be helpful to execute a command such as ping or dig from inside a cluster. By doing this it is possible to see the response, which a pod would get. This might be valuable to analyze DNS or other network connection error. Starting a BusyBox container kubectl run busybox --image=busybox:1.33 --rm -it --restart=Never /bin/sh Starting a Ubuntu container kubectl run ubuntu --image=ubuntu --rm -it --restart=Never /bin/bash Explanation run , create and run a container busybox/ubuntu , is the name of the container and can be replaced by any valid name rm , delete the container image after execution it , interactive terminal restart=Never , after termination the container should not restart; default behavior of Kubernetes","title":"Analysis"},{"location":"run-container-for-analysis/#run-a-container-for-analysis","text":"It can be helpful to execute a command such as ping or dig from inside a cluster. By doing this it is possible to see the response, which a pod would get. This might be valuable to analyze DNS or other network connection error.","title":"Run a Container for Analysis"},{"location":"run-container-for-analysis/#starting-a-busybox-container","text":"kubectl run busybox --image=busybox:1.33 --rm -it --restart=Never /bin/sh","title":"Starting a BusyBox container"},{"location":"run-container-for-analysis/#starting-a-ubuntu-container","text":"kubectl run ubuntu --image=ubuntu --rm -it --restart=Never /bin/bash","title":"Starting a Ubuntu container"},{"location":"run-container-for-analysis/#explanation","text":"run , create and run a container busybox/ubuntu , is the name of the container and can be replaced by any valid name rm , delete the container image after execution it , interactive terminal restart=Never , after termination the container should not restart; default behavior of Kubernetes","title":"Explanation"},{"location":"vagrant/","text":"Vagrant Vagrant is an open-source software to automate the provision of virtual machine. For this playground, it is used to rapidly setup a Ubuntu guest system within VirtualBox. Commands This sections covers the most important commands to use vagrant for more information use vagrant --help or man vagrant # starts and provisions the vagrant environment vagrant up # stops the vagrant environment vagrant halt # login via ssh vagrant ssh # provisions to a running vagrant machine; useful by a change of the ansible scripts vagrant provision # stops and deletes the vagrant machine vagrant destroy Ansible Vagrant offers integration for various code as infrastructure tools, among other for Ansible . In order to avoid a Ansible installation on the host system, the Vagrant ansible_local provisioner is used. Before executing the Ansible playbook, Vagrant installs Ansible on the guest system. As no Ansible installation is required on the host system, a the portability is improved, due to a reduction of dependencies. Ansible Provisioning The ansible project is in the folder *kubernetes-setup Vagrant use the Playbook in the master-playbook.yml file The role defaultSetup , installs some useful linux packages like htop or mc The role microK8s , installation of Kubernetes itself The role k8sDefaultAddons , enabling the microK8 addons DNS, Ingress and Storage by default. This should reduce re-occurring and error prone configuration tasks. By adding the addon name in the file kubernetes-setup/k8sDefaultAddons/tasks/enableK8sDefaultAddons.yml , more addons can be enabled by default. URLs https://en.wikipedia.org/wiki/Vagrant_(software)","title":"Vagrant"},{"location":"vagrant/#vagrant","text":"Vagrant is an open-source software to automate the provision of virtual machine. For this playground, it is used to rapidly setup a Ubuntu guest system within VirtualBox.","title":"Vagrant"},{"location":"vagrant/#commands","text":"This sections covers the most important commands to use vagrant for more information use vagrant --help or man vagrant # starts and provisions the vagrant environment vagrant up # stops the vagrant environment vagrant halt # login via ssh vagrant ssh # provisions to a running vagrant machine; useful by a change of the ansible scripts vagrant provision # stops and deletes the vagrant machine vagrant destroy","title":"Commands"},{"location":"vagrant/#ansible","text":"Vagrant offers integration for various code as infrastructure tools, among other for Ansible . In order to avoid a Ansible installation on the host system, the Vagrant ansible_local provisioner is used. Before executing the Ansible playbook, Vagrant installs Ansible on the guest system. As no Ansible installation is required on the host system, a the portability is improved, due to a reduction of dependencies.","title":"Ansible"},{"location":"vagrant/#ansible-provisioning","text":"The ansible project is in the folder *kubernetes-setup Vagrant use the Playbook in the master-playbook.yml file The role defaultSetup , installs some useful linux packages like htop or mc The role microK8s , installation of Kubernetes itself The role k8sDefaultAddons , enabling the microK8 addons DNS, Ingress and Storage by default. This should reduce re-occurring and error prone configuration tasks. By adding the addon name in the file kubernetes-setup/k8sDefaultAddons/tasks/enableK8sDefaultAddons.yml , more addons can be enabled by default.","title":"Ansible Provisioning"},{"location":"vagrant/#urls","text":"https://en.wikipedia.org/wiki/Vagrant_(software)","title":"URLs"},{"location":"k8s-apps/jenkins/","text":"Jenkins Operator This section will guide you through the process of installing Jenkins in the Kubernetes cluster. Enable DNS During the installation Jenkins will fetch updates, therefore DNS is needed. # Enable DNS vagrant ssh -c \"microk8s.enable dns\" Install Operator Configure Custom Resource Definition Create the custom resource definition for the jenkins operator # Apply the custom resource definition kubectl apply -f https://raw.githubusercontent.com/jenkinsci/kubernetes-operator/master/deploy/crds/jenkins_v1alpha2_jenkins_crd.yaml # Get all custom resource definition - there should be two custom resource definition: # - jenkins.jenkins.io # - jenkinsimages.jenkins.io kubectl get customresourcedefinitions.apiextensions.k8s.io # Delete the custom resource definition kubectl delete customresourcedefinitions.apiextensions.k8s.io jenkins.jenkins.io kubectl delete customresourcedefinitions.apiextensions.k8s.io jenkinsimages.jenkins.io Deploy Operator - using yaml Create the Operator in the default namespace kubectl apply -f https://raw.githubusercontent.com/jenkinsci/kubernetes-operator/master/deploy/all-in-one-v1alpha2.yaml In the default namespace should be a pod running with the name jenkins-operator-* . This can be checked with: kubectl get pods --all-namespaces Deploy Jenkins kubectl apply -f k8s-apps/jenkins/jenkins_instance.yaml After a while a jenkins pod should be running watch kubectl get pods In the case that the jenkins pod is in a restarting loop, check if DNS is enabled. # Connect to the vagrant box vagrant ssh # Check the status - dns should be listed by the enabled addons microk8s.status # or as a one liner vagrant ssh -c \"microk8s.status\" Credentials The postfix *- my-jenkins , must be same as in the jenkins_instance.yaml in the section metadata.name # The user Name of the Jenkins instance echo \"User:\"; kubectl get secret jenkins-operator-credentials-my-jenkins -o 'jsonpath={.data.user}' | base64 -d | xargs echo echo \"Password:\"; kubectl get secret jenkins-operator-credentials-my-jenkins -o 'jsonpath={.data.password}' | base64 -d | xargs echo Access the Jenkins By default Kubernetes is not routing traffic to any pods. It is possible to route your local traffic to Kubernetes by creating a port-forward # Create a port-forwarding from local host to the kubectl port-forward jenkins-my-jenkins 8080:8080 After executing this command, it is possible to open the URL http://127.0.0.1:8080 and login to Jenkins. URLs https://jenkinsci.github.io/kubernetes-operator/docs/ https://github.com/jenkinsci/kubernetes-operator The files (such as custom resource definition), are in the deploy folder","title":"Jenkins"},{"location":"k8s-apps/jenkins/#jenkins-operator","text":"This section will guide you through the process of installing Jenkins in the Kubernetes cluster.","title":"Jenkins Operator"},{"location":"k8s-apps/jenkins/#enable-dns","text":"During the installation Jenkins will fetch updates, therefore DNS is needed. # Enable DNS vagrant ssh -c \"microk8s.enable dns\"","title":"Enable DNS"},{"location":"k8s-apps/jenkins/#install-operator","text":"","title":"Install Operator"},{"location":"k8s-apps/jenkins/#configure-custom-resource-definition","text":"Create the custom resource definition for the jenkins operator # Apply the custom resource definition kubectl apply -f https://raw.githubusercontent.com/jenkinsci/kubernetes-operator/master/deploy/crds/jenkins_v1alpha2_jenkins_crd.yaml # Get all custom resource definition - there should be two custom resource definition: # - jenkins.jenkins.io # - jenkinsimages.jenkins.io kubectl get customresourcedefinitions.apiextensions.k8s.io # Delete the custom resource definition kubectl delete customresourcedefinitions.apiextensions.k8s.io jenkins.jenkins.io kubectl delete customresourcedefinitions.apiextensions.k8s.io jenkinsimages.jenkins.io","title":"Configure Custom Resource Definition"},{"location":"k8s-apps/jenkins/#deploy-operator-using-yaml","text":"Create the Operator in the default namespace kubectl apply -f https://raw.githubusercontent.com/jenkinsci/kubernetes-operator/master/deploy/all-in-one-v1alpha2.yaml In the default namespace should be a pod running with the name jenkins-operator-* . This can be checked with: kubectl get pods --all-namespaces","title":"Deploy Operator - using yaml"},{"location":"k8s-apps/jenkins/#deploy-jenkins","text":"kubectl apply -f k8s-apps/jenkins/jenkins_instance.yaml After a while a jenkins pod should be running watch kubectl get pods In the case that the jenkins pod is in a restarting loop, check if DNS is enabled. # Connect to the vagrant box vagrant ssh # Check the status - dns should be listed by the enabled addons microk8s.status # or as a one liner vagrant ssh -c \"microk8s.status\"","title":"Deploy Jenkins"},{"location":"k8s-apps/jenkins/#credentials","text":"The postfix *- my-jenkins , must be same as in the jenkins_instance.yaml in the section metadata.name # The user Name of the Jenkins instance echo \"User:\"; kubectl get secret jenkins-operator-credentials-my-jenkins -o 'jsonpath={.data.user}' | base64 -d | xargs echo echo \"Password:\"; kubectl get secret jenkins-operator-credentials-my-jenkins -o 'jsonpath={.data.password}' | base64 -d | xargs echo","title":"Credentials"},{"location":"k8s-apps/jenkins/#access-the-jenkins","text":"By default Kubernetes is not routing traffic to any pods. It is possible to route your local traffic to Kubernetes by creating a port-forward # Create a port-forwarding from local host to the kubectl port-forward jenkins-my-jenkins 8080:8080 After executing this command, it is possible to open the URL http://127.0.0.1:8080 and login to Jenkins.","title":"Access the Jenkins"},{"location":"k8s-apps/jenkins/#urls","text":"https://jenkinsci.github.io/kubernetes-operator/docs/ https://github.com/jenkinsci/kubernetes-operator The files (such as custom resource definition), are in the deploy folder","title":"URLs"},{"location":"k8s-apps/nginx/","text":"Ngnix Deployment Diagram Deploy Ngnix Deployment In the first step three Nginx instances are deployed to the Kubernetes Cluster . All three instances providing the same content, a simple \"Hello World\" # Creates the deployment and rollout three nginx instances kubectl apply -f k8s-apps/nginx/nginx-deployment.yaml Next the nginx-service is deployed, this is a kubernetes resource to expose the pods to the internal network of the cluster . Service # Create a service which will route the traffic to one of the three nginx pods kubectl apply -f k8s-apps/nginx/nginx-service.yaml The pods are dynamically deployments, which means that pods can be created and destroyed at anytime. Consequently, the network configuration such as IP Addresses will change. Besides, all three pods are serving the same content and it is just relevant which pod will handle the request. A Kubernetes Service solve this by giving as a stable DNS name and ensuring that the request it routed to one of the three pods. Testing the Service Services are just accessible within the Cluster , thus to test the configuration it is needed to create a Port-Forward from the client to the Kubernetes Cluster . # Create a port-forwarding from local host to the service kubectl port-forward service/my-ngnix-service 8080:8080 Opening http://192.168.60.10:8080 in a browser should show us the default Nginx start page. Ingress The goal of the last step is to expose the Nginx to the outside. Network traffic which is entering a network is called ingress , traffic leaving a network is called egress . A Kubernetes Ingress consists of an Ingress Controller and an Ingress Resources . The controller is in a nutshell a http proxy, which will forward the traffic to the service/pod. In contrast is the resource the configuration of the controller behavior. It is possible to use the same controller for several resources. # Enable a Ingress controller vagrant ssh -c \"microk8s.enable ingress\" # Create a ingress kubectl apply -f k8s-apps/nginx/nginx-ingress.yaml The nginx is now reachable from outside the cluster, to test it open http://192.168.60.10/path-of-my-ingress in a Browser. How related resources are tied The pod deployment resource file nginx-deployment.yaml , specifics labels. These labels are useful to select a set of pods, e.g. for searching pods or linking resources together. In the service resource file nginx-service.yaml the selector directive is used to select all pods with a specific set of labels. In this example the label app: nginx is used. The name directive is used in the ingress resource nginx-service.yaml , to connect the ingress with the service. Routing Network traffic The nginx-ingress accept http request on port 80 from the client. All traffic on port 80 is forwarded from the nginx-ingress to the nginx-service on Port 8080 The request which the nginx-service received on port 8080 is forwarded to exactly one nginx-pod on port 80 URLs https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ https://kubernetes.io/docs/concepts/services-networking/service/ https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/ https://kubernetes.io/docs/concepts/services-networking/ingress/","title":"Nginx"},{"location":"k8s-apps/nginx/#ngnix","text":"","title":"Ngnix"},{"location":"k8s-apps/nginx/#deployment-diagram","text":"","title":"Deployment Diagram"},{"location":"k8s-apps/nginx/#deploy-ngnix","text":"","title":"Deploy Ngnix"},{"location":"k8s-apps/nginx/#deployment","text":"In the first step three Nginx instances are deployed to the Kubernetes Cluster . All three instances providing the same content, a simple \"Hello World\" # Creates the deployment and rollout three nginx instances kubectl apply -f k8s-apps/nginx/nginx-deployment.yaml Next the nginx-service is deployed, this is a kubernetes resource to expose the pods to the internal network of the cluster .","title":"Deployment"},{"location":"k8s-apps/nginx/#service","text":"# Create a service which will route the traffic to one of the three nginx pods kubectl apply -f k8s-apps/nginx/nginx-service.yaml The pods are dynamically deployments, which means that pods can be created and destroyed at anytime. Consequently, the network configuration such as IP Addresses will change. Besides, all three pods are serving the same content and it is just relevant which pod will handle the request. A Kubernetes Service solve this by giving as a stable DNS name and ensuring that the request it routed to one of the three pods.","title":"Service"},{"location":"k8s-apps/nginx/#testing-the-service","text":"Services are just accessible within the Cluster , thus to test the configuration it is needed to create a Port-Forward from the client to the Kubernetes Cluster . # Create a port-forwarding from local host to the service kubectl port-forward service/my-ngnix-service 8080:8080 Opening http://192.168.60.10:8080 in a browser should show us the default Nginx start page.","title":"Testing the Service"},{"location":"k8s-apps/nginx/#ingress","text":"The goal of the last step is to expose the Nginx to the outside. Network traffic which is entering a network is called ingress , traffic leaving a network is called egress . A Kubernetes Ingress consists of an Ingress Controller and an Ingress Resources . The controller is in a nutshell a http proxy, which will forward the traffic to the service/pod. In contrast is the resource the configuration of the controller behavior. It is possible to use the same controller for several resources. # Enable a Ingress controller vagrant ssh -c \"microk8s.enable ingress\" # Create a ingress kubectl apply -f k8s-apps/nginx/nginx-ingress.yaml The nginx is now reachable from outside the cluster, to test it open http://192.168.60.10/path-of-my-ingress in a Browser.","title":"Ingress"},{"location":"k8s-apps/nginx/#how-related-resources-are-tied","text":"The pod deployment resource file nginx-deployment.yaml , specifics labels. These labels are useful to select a set of pods, e.g. for searching pods or linking resources together. In the service resource file nginx-service.yaml the selector directive is used to select all pods with a specific set of labels. In this example the label app: nginx is used. The name directive is used in the ingress resource nginx-service.yaml , to connect the ingress with the service.","title":"How related resources are tied"},{"location":"k8s-apps/nginx/#routing-network-traffic","text":"The nginx-ingress accept http request on port 80 from the client. All traffic on port 80 is forwarded from the nginx-ingress to the nginx-service on Port 8080 The request which the nginx-service received on port 8080 is forwarded to exactly one nginx-pod on port 80","title":"Routing Network traffic"},{"location":"k8s-apps/nginx/#urls","text":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ https://kubernetes.io/docs/concepts/services-networking/service/ https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/ https://kubernetes.io/docs/concepts/services-networking/ingress/","title":"URLs"},{"location":"k8s-apps/fluxv2/fluxv2-cli/","text":"FluxV2 CLI Install the flux cli curl -s https://fluxcd.io/install.sh | sudo bash Enable Autocompletion - Bash vi ~/.bashrc # Add this line . <(flux completion bash) Enable Autocompletion - Zsh vi ~/.zshrc # Add this line source <(flux completion zsh) && compdef _flux flux","title":"CLI"},{"location":"k8s-apps/fluxv2/fluxv2-cli/#fluxv2-cli","text":"","title":"FluxV2 CLI"},{"location":"k8s-apps/fluxv2/fluxv2-cli/#install-the-flux-cli","text":"curl -s https://fluxcd.io/install.sh | sudo bash","title":"Install the flux cli"},{"location":"k8s-apps/fluxv2/fluxv2-cli/#enable-autocompletion-bash","text":"vi ~/.bashrc # Add this line . <(flux completion bash)","title":"Enable Autocompletion - Bash"},{"location":"k8s-apps/fluxv2/fluxv2-cli/#enable-autocompletion-zsh","text":"vi ~/.zshrc # Add this line source <(flux completion zsh) && compdef _flux flux","title":"Enable Autocompletion - Zsh"},{"location":"k8s-apps/fluxv2/fluxv2-deploy-podinfo-helm/","text":"Deploy Podinfo - Helm Podinfo is a microservice to demonstrate best practices of running a microservice in a Kubernetes cluster. In this chapter Podinfo is used a deployment unit and deployed with FluxV2 kustomization controller. Prerequisites FluxV2 installed in a Kubernetes cluster A septate configuration git repository, the repository from the installation section is sufficient Adding Git repository as flux source First, the flux configuration repository must be cloned to the local computer. git clone git@github.com:zonedbit/flux-conf-playground.git git checkout flux-conf cd flux-conf-playground In the next step a CRD is created, this is a representing the helm repository which should be deployed to Kubernetes cluster. flux create source helm podinfo \\ --namespace=default \\ --url=https://stefanprodan.github.io/podinfo \\ --interval=10m \\ --export > ./clusters/green/podinfo-source-helm.yaml After running this command a CRD is created, and can be pushed to the git repository. git add -A && git commit -m \"Add podinfo HelmRepository\" git push A moment later the helm repository should appear as a new CRD and pointing to the HELM repository of Podinfo flux get sources helm -A NAMESPACE NAME READY MESSAGE REVISION SUSPENDED default podinfo True Fetched revision: 8411f23d07d3701f0e96e7d9e503b7936d7e1d56 8411f23d07d3701f0e96e7d9e503b7936d7e1d56 False Deploy Podinfo via Helm In the first step a simple value yaml is create to configure the helm chart. cat > ./clusters/green/podinfo-values.yaml <<EOL replicaCount: 4 resources: limits: memory: 256Mi requests: cpu: 100m memory: 64Mi EOL Now the Podinfo microservie will be deployed to the Kubernetes cluster. Therefor the flux cli is used to create a helmrelease CRD. Pushing the Podinfo helmrelease CRD to the git repository, leads to the deployment of the Podinfo microservice. flux create helmrelease podinfo \\ --namespace=default \\ --source=HelmRepository/podinfo \\ --release-name=podinfo \\ --chart=podinfo \\ --chart-version=\">5.0.0\" \\ --values=./clusters/green/podinfo-values.yaml \\ --export > ./clusters/green/podinfo-helmrelease.yaml git add -A && git commit -m \"Add podinfo helmrelease\" git push By creating a port-forward it is possible to access Podinfo in a Browser via http://127.0.0.1:8080 kubectl port-forward service/podinfo 8080:9898 URLs https://github.com/stefanprodan/podinfo","title":"Deploy App (Helm)"},{"location":"k8s-apps/fluxv2/fluxv2-deploy-podinfo-helm/#deploy-podinfo-helm","text":"Podinfo is a microservice to demonstrate best practices of running a microservice in a Kubernetes cluster. In this chapter Podinfo is used a deployment unit and deployed with FluxV2 kustomization controller.","title":"Deploy Podinfo - Helm"},{"location":"k8s-apps/fluxv2/fluxv2-deploy-podinfo-helm/#prerequisites","text":"FluxV2 installed in a Kubernetes cluster A septate configuration git repository, the repository from the installation section is sufficient","title":"Prerequisites"},{"location":"k8s-apps/fluxv2/fluxv2-deploy-podinfo-helm/#adding-git-repository-as-flux-source","text":"First, the flux configuration repository must be cloned to the local computer. git clone git@github.com:zonedbit/flux-conf-playground.git git checkout flux-conf cd flux-conf-playground In the next step a CRD is created, this is a representing the helm repository which should be deployed to Kubernetes cluster. flux create source helm podinfo \\ --namespace=default \\ --url=https://stefanprodan.github.io/podinfo \\ --interval=10m \\ --export > ./clusters/green/podinfo-source-helm.yaml After running this command a CRD is created, and can be pushed to the git repository. git add -A && git commit -m \"Add podinfo HelmRepository\" git push A moment later the helm repository should appear as a new CRD and pointing to the HELM repository of Podinfo flux get sources helm -A NAMESPACE NAME READY MESSAGE REVISION SUSPENDED default podinfo True Fetched revision: 8411f23d07d3701f0e96e7d9e503b7936d7e1d56 8411f23d07d3701f0e96e7d9e503b7936d7e1d56 False","title":"Adding Git repository as flux source"},{"location":"k8s-apps/fluxv2/fluxv2-deploy-podinfo-helm/#deploy-podinfo-via-helm","text":"In the first step a simple value yaml is create to configure the helm chart. cat > ./clusters/green/podinfo-values.yaml <<EOL replicaCount: 4 resources: limits: memory: 256Mi requests: cpu: 100m memory: 64Mi EOL Now the Podinfo microservie will be deployed to the Kubernetes cluster. Therefor the flux cli is used to create a helmrelease CRD. Pushing the Podinfo helmrelease CRD to the git repository, leads to the deployment of the Podinfo microservice. flux create helmrelease podinfo \\ --namespace=default \\ --source=HelmRepository/podinfo \\ --release-name=podinfo \\ --chart=podinfo \\ --chart-version=\">5.0.0\" \\ --values=./clusters/green/podinfo-values.yaml \\ --export > ./clusters/green/podinfo-helmrelease.yaml git add -A && git commit -m \"Add podinfo helmrelease\" git push By creating a port-forward it is possible to access Podinfo in a Browser via http://127.0.0.1:8080 kubectl port-forward service/podinfo 8080:9898","title":"Deploy Podinfo via Helm"},{"location":"k8s-apps/fluxv2/fluxv2-deploy-podinfo-helm/#urls","text":"https://github.com/stefanprodan/podinfo","title":"URLs"},{"location":"k8s-apps/fluxv2/fluxv2-deploy-podinfo/","text":"Deploy Podinfo Podinfo is a microservice to demonstrate best practices of running a microservice in a Kubernetes cluster. In this chapter Podinfo is used a deployment unit and deployed with FluxV2 kustomization controller. Prerequisites FluxV2 installed in a Kubernetes cluster A septate configuration git repository, the repository from the installation section is sufficient Adding Git repository as flux source First, the flux configuration repository must be cloned to the local computer. git clone git@github.com:zonedbit/flux-conf-playground.git git checkout flux-conf cd flux-conf-playground In the next step a CRD is created, this is a representing the git repository which should be deployed to Kubernetes cluster. flux create source git podinfo \\ --url=https://github.com/stefanprodan/podinfo \\ --branch=master \\ --interval=30s \\ --export > ./clusters/green/podinfo-source.yaml After running this command a CRD is created, and can be pushed to the git repository. git add -A && git commit -m \"Add podinfo GitRepository\" git push A moment later the git repository should appear as a new CRD and pointing to the GitHub repository of Podinfo kubectl get gitrepositories.source.toolkit.fluxcd.io --all-namespaces NAMESPACE NAME URL READY STATUS AGE flux-system podinfo https://github.com/stefanprodan/podinfo True Fetched revision: master/627d5c4bb67b77185f37e31d734b085019ff2951 51m flux-system flux-system ssh://git@github.com/zonedbit/flux-conf-playground.git True Fetched revision: flux-conf/70d9f421bb9347101c52dd20580fcbeddd2a218a 65m Deploy Podinfo via kustomization Now the Podinfo microservie will be deployed to the Kubernetes cluster. Therefor the flux cli is used to create a kustomization CRD. Pushing the Podinfo kustomization CRD to the git repository, leads to the deployment of the Podinfo microservice. flux create kustomization podinfo \\ --source=podinfo \\ --path=\"./kustomize\" \\ --prune=true \\ --validation=client \\ --interval=5m \\ --export > ./clusters/green/podinfo-kustomization.yaml git add -A && git commit -m \"Add podinfo Kustomization\" git push By creating a port-forward it is possible to access Podinfo in a Browser via http://127.0.0.1:8080 kubectl port-forward service/podinfo 8080:9898 URLs https://github.com/stefanprodan/podinfo","title":"Deploy App"},{"location":"k8s-apps/fluxv2/fluxv2-deploy-podinfo/#deploy-podinfo","text":"Podinfo is a microservice to demonstrate best practices of running a microservice in a Kubernetes cluster. In this chapter Podinfo is used a deployment unit and deployed with FluxV2 kustomization controller.","title":"Deploy Podinfo"},{"location":"k8s-apps/fluxv2/fluxv2-deploy-podinfo/#prerequisites","text":"FluxV2 installed in a Kubernetes cluster A septate configuration git repository, the repository from the installation section is sufficient","title":"Prerequisites"},{"location":"k8s-apps/fluxv2/fluxv2-deploy-podinfo/#adding-git-repository-as-flux-source","text":"First, the flux configuration repository must be cloned to the local computer. git clone git@github.com:zonedbit/flux-conf-playground.git git checkout flux-conf cd flux-conf-playground In the next step a CRD is created, this is a representing the git repository which should be deployed to Kubernetes cluster. flux create source git podinfo \\ --url=https://github.com/stefanprodan/podinfo \\ --branch=master \\ --interval=30s \\ --export > ./clusters/green/podinfo-source.yaml After running this command a CRD is created, and can be pushed to the git repository. git add -A && git commit -m \"Add podinfo GitRepository\" git push A moment later the git repository should appear as a new CRD and pointing to the GitHub repository of Podinfo kubectl get gitrepositories.source.toolkit.fluxcd.io --all-namespaces NAMESPACE NAME URL READY STATUS AGE flux-system podinfo https://github.com/stefanprodan/podinfo True Fetched revision: master/627d5c4bb67b77185f37e31d734b085019ff2951 51m flux-system flux-system ssh://git@github.com/zonedbit/flux-conf-playground.git True Fetched revision: flux-conf/70d9f421bb9347101c52dd20580fcbeddd2a218a 65m","title":"Adding Git repository as flux source"},{"location":"k8s-apps/fluxv2/fluxv2-deploy-podinfo/#deploy-podinfo-via-kustomization","text":"Now the Podinfo microservie will be deployed to the Kubernetes cluster. Therefor the flux cli is used to create a kustomization CRD. Pushing the Podinfo kustomization CRD to the git repository, leads to the deployment of the Podinfo microservice. flux create kustomization podinfo \\ --source=podinfo \\ --path=\"./kustomize\" \\ --prune=true \\ --validation=client \\ --interval=5m \\ --export > ./clusters/green/podinfo-kustomization.yaml git add -A && git commit -m \"Add podinfo Kustomization\" git push By creating a port-forward it is possible to access Podinfo in a Browser via http://127.0.0.1:8080 kubectl port-forward service/podinfo 8080:9898","title":"Deploy Podinfo via kustomization"},{"location":"k8s-apps/fluxv2/fluxv2-deploy-podinfo/#urls","text":"https://github.com/stefanprodan/podinfo","title":"URLs"},{"location":"k8s-apps/fluxv2/fluxv2-installation/","text":"Install Flux V2 Prerequisites A Kubernetes cluster to install FluxV2 A Git Repository to manage the FluxV2 configuration This repository is used by FluxV2 to manage is own configuration. For testing purpose, the easy way is to create a private GitHub repository Verify the cluster # Ensure that the cluster fullfil the flux prerequisites. flux check --pre Install FluxV2 In the next command replace the URL against your own repository In URL provided by GitHub to clone the repository must the : after the top-level domain com replaced against a / During the installation FluxV2 will generate a pair of private and public ssh keys The public key is printed to the terminal and before continuing the installation the key must be added to the GitHub repo Go to GitHub repository -> Settings -> Deploy Keys -> Add deploy keys Not sure if needed, just give FluxV2 write access to the repository. It is a playground to make things easy and not production ready. # Replace the URL with the your configuration repository flux bootstrap git \\ --url=ssh://git@github.com/zonedbit/flux-conf-playground.git \\ --branch=flux-conf \\ --path=clusters/green After a moment and executing the command flux check you should see a similar result. \u279c flux check \u25ba checking prerequisites \u2714 kubectl 1.21.1 >=1.18.0-0 \u2714 Kubernetes 1.21.1-3+ba118484dd39df >=1.16.0-0 \u25ba checking controllers \u2714 helm-controller: deployment ready \u25ba ghcr.io/fluxcd/helm-controller:v0.11.1 \u2714 kustomize-controller: deployment ready \u25ba ghcr.io/fluxcd/kustomize-controller:v0.13.2 \u2714 notification-controller: deployment ready \u25ba ghcr.io/fluxcd/notification-controller:v0.15.0 \u2714 source-controller: deployment ready \u25ba ghcr.io/fluxcd/source-controller:v0.15.3 \u2714 all checks passed URLs https://fluxcd.io/docs/","title":"Installation"},{"location":"k8s-apps/fluxv2/fluxv2-installation/#install-flux-v2","text":"","title":"Install Flux V2"},{"location":"k8s-apps/fluxv2/fluxv2-installation/#prerequisites","text":"A Kubernetes cluster to install FluxV2 A Git Repository to manage the FluxV2 configuration This repository is used by FluxV2 to manage is own configuration. For testing purpose, the easy way is to create a private GitHub repository","title":"Prerequisites"},{"location":"k8s-apps/fluxv2/fluxv2-installation/#verify-the-cluster","text":"# Ensure that the cluster fullfil the flux prerequisites. flux check --pre","title":"Verify the cluster"},{"location":"k8s-apps/fluxv2/fluxv2-installation/#install-fluxv2","text":"In the next command replace the URL against your own repository In URL provided by GitHub to clone the repository must the : after the top-level domain com replaced against a / During the installation FluxV2 will generate a pair of private and public ssh keys The public key is printed to the terminal and before continuing the installation the key must be added to the GitHub repo Go to GitHub repository -> Settings -> Deploy Keys -> Add deploy keys Not sure if needed, just give FluxV2 write access to the repository. It is a playground to make things easy and not production ready. # Replace the URL with the your configuration repository flux bootstrap git \\ --url=ssh://git@github.com/zonedbit/flux-conf-playground.git \\ --branch=flux-conf \\ --path=clusters/green After a moment and executing the command flux check you should see a similar result. \u279c flux check \u25ba checking prerequisites \u2714 kubectl 1.21.1 >=1.18.0-0 \u2714 Kubernetes 1.21.1-3+ba118484dd39df >=1.16.0-0 \u25ba checking controllers \u2714 helm-controller: deployment ready \u25ba ghcr.io/fluxcd/helm-controller:v0.11.1 \u2714 kustomize-controller: deployment ready \u25ba ghcr.io/fluxcd/kustomize-controller:v0.13.2 \u2714 notification-controller: deployment ready \u25ba ghcr.io/fluxcd/notification-controller:v0.15.0 \u2714 source-controller: deployment ready \u25ba ghcr.io/fluxcd/source-controller:v0.15.3 \u2714 all checks passed","title":"Install FluxV2"},{"location":"k8s-apps/fluxv2/fluxv2-installation/#urls","text":"https://fluxcd.io/docs/","title":"URLs"},{"location":"k8s-apps/fluxv2/fluxv2-repository-structure/","text":"Repository Structure Create configuration repository Create a github repository, which can be used to store the configuration. # Move to the temp folder cd /tmp # Clone the flux configuration repository git clone git@github.com:zonedbit/flux-conf-playground.git # Copy the configuration files to the repository # Assuming the working directory is the kubernetes-playground folder cp -r ./k8s-apps/flux-repository-structure-example/* /tmp/flux-conf-playground/ # Push the configuration to the github repository git add -A && git commit -m \"Add environment and podinfo configuration\" && git push Configuration Repository Structure At the top level the configuration repository contains three apps, clusters, and infrastructure folders. In the next sub-section the meaning of these folders are explained. . \u251c\u2500\u2500 apps \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u2514\u2500\u2500 podinfo \u2502 \u2502 \u251c\u2500\u2500 ingress.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace.yaml \u2502 \u2502 \u2514\u2500\u2500 release.yaml \u2502 \u2514\u2500\u2500 staging \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 podinfo-values.yaml \u251c\u2500\u2500 clusters \u2502 \u2514\u2500\u2500 staging \u2502 \u251c\u2500\u2500 apps.yaml \u2502 \u2514\u2500\u2500 infrastructure.yaml \u2514\u2500\u2500 infrastructure \u2514\u2500\u2500 sources \u251c\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 podinfo-source-helm.yaml clusters The clusters folder contains one folder for each cluster or environment, which should be managed by FluxV2 . During the bootstrapping of FluxV2 , the path (entry point) for flux is set to the folder clusters/staging The infrastructure.yaml points to the infrastructure folder. Hence, flux will apply the kustomization in the infrastructure folder The apps.yaml points to the apps/staging overlay of kustomize. As the app.yaml defines a dependency to the infrastructure.yaml , flux will apply the apps.yaml after the infrastructure.yaml . infrastructure In this example the infrastructure folders contains one sub-folder sources . The sources folder, contains a pointer to a helm repository. This helm repository is used for pulling the app into the cluster apps The apps is in a nutshell the configuration of the app/microserive based on kustomize The base folder contains the podinfo folder, which is in this example the unit of deployment Here is the default configuration stored The namespace.yaml , creates the namespace podinfo The release.yaml , installs the podinfo helm chart to the cluster The ingress.yaml , creates an ingress resource, so that ingress traffic is routed to the pod The staging folder, is a kustomize overlay. This overlay points to the base configuration and applies some cluster specific configuration. Bootstrap the Staging Environment In the last step the staging environment is bootstrapped, here it is important the path parameter points to the folder ./clusters/staging . # Bootstrap Flux flux bootstrap git \\ --url=ssh://git@github.com/zonedbit/flux-conf-playground.git \\ --branch=main \\ --path=./clusters/staging URLs http://podinfo.k8s.home https://github.com/fluxcd/flux2-kustomize-helm-example","title":"Repository Structure"},{"location":"k8s-apps/fluxv2/fluxv2-repository-structure/#repository-structure","text":"","title":"Repository Structure"},{"location":"k8s-apps/fluxv2/fluxv2-repository-structure/#create-configuration-repository","text":"Create a github repository, which can be used to store the configuration. # Move to the temp folder cd /tmp # Clone the flux configuration repository git clone git@github.com:zonedbit/flux-conf-playground.git # Copy the configuration files to the repository # Assuming the working directory is the kubernetes-playground folder cp -r ./k8s-apps/flux-repository-structure-example/* /tmp/flux-conf-playground/ # Push the configuration to the github repository git add -A && git commit -m \"Add environment and podinfo configuration\" && git push","title":"Create configuration repository"},{"location":"k8s-apps/fluxv2/fluxv2-repository-structure/#configuration-repository-structure","text":"At the top level the configuration repository contains three apps, clusters, and infrastructure folders. In the next sub-section the meaning of these folders are explained. . \u251c\u2500\u2500 apps \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u2514\u2500\u2500 podinfo \u2502 \u2502 \u251c\u2500\u2500 ingress.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace.yaml \u2502 \u2502 \u2514\u2500\u2500 release.yaml \u2502 \u2514\u2500\u2500 staging \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 podinfo-values.yaml \u251c\u2500\u2500 clusters \u2502 \u2514\u2500\u2500 staging \u2502 \u251c\u2500\u2500 apps.yaml \u2502 \u2514\u2500\u2500 infrastructure.yaml \u2514\u2500\u2500 infrastructure \u2514\u2500\u2500 sources \u251c\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 podinfo-source-helm.yaml","title":"Configuration Repository Structure"},{"location":"k8s-apps/fluxv2/fluxv2-repository-structure/#clusters","text":"The clusters folder contains one folder for each cluster or environment, which should be managed by FluxV2 . During the bootstrapping of FluxV2 , the path (entry point) for flux is set to the folder clusters/staging The infrastructure.yaml points to the infrastructure folder. Hence, flux will apply the kustomization in the infrastructure folder The apps.yaml points to the apps/staging overlay of kustomize. As the app.yaml defines a dependency to the infrastructure.yaml , flux will apply the apps.yaml after the infrastructure.yaml .","title":"clusters"},{"location":"k8s-apps/fluxv2/fluxv2-repository-structure/#infrastructure","text":"In this example the infrastructure folders contains one sub-folder sources . The sources folder, contains a pointer to a helm repository. This helm repository is used for pulling the app into the cluster","title":"infrastructure"},{"location":"k8s-apps/fluxv2/fluxv2-repository-structure/#apps","text":"The apps is in a nutshell the configuration of the app/microserive based on kustomize The base folder contains the podinfo folder, which is in this example the unit of deployment Here is the default configuration stored The namespace.yaml , creates the namespace podinfo The release.yaml , installs the podinfo helm chart to the cluster The ingress.yaml , creates an ingress resource, so that ingress traffic is routed to the pod The staging folder, is a kustomize overlay. This overlay points to the base configuration and applies some cluster specific configuration.","title":"apps"},{"location":"k8s-apps/fluxv2/fluxv2-repository-structure/#bootstrap-the-staging-environment","text":"In the last step the staging environment is bootstrapped, here it is important the path parameter points to the folder ./clusters/staging . # Bootstrap Flux flux bootstrap git \\ --url=ssh://git@github.com/zonedbit/flux-conf-playground.git \\ --branch=main \\ --path=./clusters/staging","title":"Bootstrap the Staging Environment"},{"location":"k8s-apps/fluxv2/fluxv2-repository-structure/#urls","text":"http://podinfo.k8s.home https://github.com/fluxcd/flux2-kustomize-helm-example","title":"URLs"},{"location":"k8s-apps/tekton/tekton-installation/","text":"Tekton Installation Tekton an cloud native open-source framework for creating CI/CD systems. The next steps are describing the installation process on the Kubernetes-playground. Install Pipelines Install the core component Tekton Pipelines kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml After a moment Tekton Pipline should run in his own namespace. # Check the status of the tekton pods kubectl get pods --namespace tekton-pipelines # or use watch for auto refresh watch kubectl get pods --namespace tekton-pipelines Tekton CLI Install tkn the Tekton command line interface. sudo apt update sudo apt install -y gnupg sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3EFE0E0A2F2F60AA echo \"deb http://ppa.launchpad.net/tektoncd/cli/ubuntu eoan main\"|sudo tee /etc/apt/sources.list.d/tektoncd-ubuntu-cli.list sudo apt update && sudo apt install -y tektoncd-cli Tekton Dashboard kubectl apply --filename https://github.com/tektoncd/dashboard/releases/latest/download/tekton-dashboard-release.yaml Access via port-forward After the setup of a port-forward to the tekton-dashboard service, it is possible to access the Dashboard in your Browser with the URL http://127.0.0.1:9097 kubectl --namespace tekton-pipelines port-forward svc/tekton-dashboard 9097:9097 Ingress Next the Tekton Dashboard is exposed to the outside. Therefore, a ingress configuration is created. The concept of ingress controllers are explained in the Nginx chapter. # Enable a Ingress controller vagrant ssh -c \"microk8s.enable ingress\" # Create a ingress kubectl apply -f k8s-apps/tekton/tekton-dashboard-ingress.yaml The tekton-dashboard is now reachable from outside of the cluster, via the URL http://tekton.k8s.home . Instead of the external cluster IP Address, a sub-domain is used to access the tekton-dashboard . This makes the ingress configuration easier, when a web-application loads resources by their absolute URL; otherwise the URL rewriting in the ingress configuration gets challenging. For details, see the network configuration chapter. Hello Tekton Task In the next steps a simple hello world task is created, executed and the result is checked. The taskrun is visible on the shell and the dashboard. # Create the hello task in the default namespace kubectl apply -f k8s-apps/tekton/hello-task.yaml # Dry run from the shell tkn task start hello --dry-run # Start the command form the shell tkn task start hello # View the logs of the last taskrun tkn taskrun logs --last -f URLs https://tekton.dev https://tekton.dev/docs/getting-started/#installation","title":"Installation"},{"location":"k8s-apps/tekton/tekton-installation/#tekton-installation","text":"Tekton an cloud native open-source framework for creating CI/CD systems. The next steps are describing the installation process on the Kubernetes-playground.","title":"Tekton Installation"},{"location":"k8s-apps/tekton/tekton-installation/#install-pipelines","text":"Install the core component Tekton Pipelines kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml After a moment Tekton Pipline should run in his own namespace. # Check the status of the tekton pods kubectl get pods --namespace tekton-pipelines # or use watch for auto refresh watch kubectl get pods --namespace tekton-pipelines","title":"Install Pipelines"},{"location":"k8s-apps/tekton/tekton-installation/#tekton-cli","text":"Install tkn the Tekton command line interface. sudo apt update sudo apt install -y gnupg sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3EFE0E0A2F2F60AA echo \"deb http://ppa.launchpad.net/tektoncd/cli/ubuntu eoan main\"|sudo tee /etc/apt/sources.list.d/tektoncd-ubuntu-cli.list sudo apt update && sudo apt install -y tektoncd-cli","title":"Tekton CLI"},{"location":"k8s-apps/tekton/tekton-installation/#tekton-dashboard","text":"kubectl apply --filename https://github.com/tektoncd/dashboard/releases/latest/download/tekton-dashboard-release.yaml","title":"Tekton Dashboard"},{"location":"k8s-apps/tekton/tekton-installation/#access-via-port-forward","text":"After the setup of a port-forward to the tekton-dashboard service, it is possible to access the Dashboard in your Browser with the URL http://127.0.0.1:9097 kubectl --namespace tekton-pipelines port-forward svc/tekton-dashboard 9097:9097","title":"Access via port-forward"},{"location":"k8s-apps/tekton/tekton-installation/#ingress","text":"Next the Tekton Dashboard is exposed to the outside. Therefore, a ingress configuration is created. The concept of ingress controllers are explained in the Nginx chapter. # Enable a Ingress controller vagrant ssh -c \"microk8s.enable ingress\" # Create a ingress kubectl apply -f k8s-apps/tekton/tekton-dashboard-ingress.yaml The tekton-dashboard is now reachable from outside of the cluster, via the URL http://tekton.k8s.home . Instead of the external cluster IP Address, a sub-domain is used to access the tekton-dashboard . This makes the ingress configuration easier, when a web-application loads resources by their absolute URL; otherwise the URL rewriting in the ingress configuration gets challenging. For details, see the network configuration chapter.","title":"Ingress"},{"location":"k8s-apps/tekton/tekton-installation/#hello-tekton-task","text":"In the next steps a simple hello world task is created, executed and the result is checked. The taskrun is visible on the shell and the dashboard. # Create the hello task in the default namespace kubectl apply -f k8s-apps/tekton/hello-task.yaml # Dry run from the shell tkn task start hello --dry-run # Start the command form the shell tkn task start hello # View the logs of the last taskrun tkn taskrun logs --last -f","title":"Hello Tekton Task"},{"location":"k8s-apps/tekton/tekton-installation/#urls","text":"https://tekton.dev https://tekton.dev/docs/getting-started/#installation","title":"URLs"},{"location":"k8s-apps/tekton/tekton-pipeline/","text":"Tekton Pipeline In this next steps a Tekton Task and Pipeline is configured to build the Spring PetClinic Sample Application . As the Spring PetClinic is a Maven based project, a persistent volume claim is configured, to cache the dependencies. Prepare the cluster # Enable storage for a persistence volume claim, to cache the maven dependencies vagrant ssh -c \"microk8s.enable storage\" # Enable DNS, to resolve domain names during the build vagrant ssh -c \"microk8s.enable dns\" Create a PersistentVolumeClaim First, the persistence volume claim is created, to cache the maven dependencies. From the second build, a download of the dependencies can avoid and the build will gain speed, as . kubectl apply -f k8s-apps/tekton/pipeline/maven-repo-pvc.yaml Create a Task A task is a series of steps , which are executed in the same kubernetes pod. Each step runs in separate container in the same pod. This maven task is independent form a specific maven project and can be reused across multiple maven project. kubectl apply -f k8s-apps/tekton/pipeline/maven-task.yaml Create a Pipeline Pipelines are a group of multiple tasks, with the key characteristic: Each pipeline is executed is a separate pod Multiple pipelines are connected by a directed acyclic graph (DAG) Pipelines are executed in the graph order kubectl apply -f k8s-apps/tekton/pipeline/maven-build-pipeline.yaml Start a pipeline run The Pipeline is like a class in object oriented programming and abstract description, without specific values. A PipelineRun is the instantiation and execution of a Pipeline with specific values. kubectl create -f k8s-apps/tekton/pipeline/run/spring-petclinic-pipeline-run.yaml URLs https://tekton.dev/docs/concepts/ https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://developers.redhat.com/blog/2020/02/26/speed-up-maven-builds-in-tekton-pipelines/","title":"Pipeline"},{"location":"k8s-apps/tekton/tekton-pipeline/#tekton-pipeline","text":"In this next steps a Tekton Task and Pipeline is configured to build the Spring PetClinic Sample Application . As the Spring PetClinic is a Maven based project, a persistent volume claim is configured, to cache the dependencies.","title":"Tekton Pipeline"},{"location":"k8s-apps/tekton/tekton-pipeline/#prepare-the-cluster","text":"# Enable storage for a persistence volume claim, to cache the maven dependencies vagrant ssh -c \"microk8s.enable storage\" # Enable DNS, to resolve domain names during the build vagrant ssh -c \"microk8s.enable dns\"","title":"Prepare the cluster"},{"location":"k8s-apps/tekton/tekton-pipeline/#create-a-persistentvolumeclaim","text":"First, the persistence volume claim is created, to cache the maven dependencies. From the second build, a download of the dependencies can avoid and the build will gain speed, as . kubectl apply -f k8s-apps/tekton/pipeline/maven-repo-pvc.yaml","title":"Create a PersistentVolumeClaim"},{"location":"k8s-apps/tekton/tekton-pipeline/#create-a-task","text":"A task is a series of steps , which are executed in the same kubernetes pod. Each step runs in separate container in the same pod. This maven task is independent form a specific maven project and can be reused across multiple maven project. kubectl apply -f k8s-apps/tekton/pipeline/maven-task.yaml","title":"Create a Task"},{"location":"k8s-apps/tekton/tekton-pipeline/#create-a-pipeline","text":"Pipelines are a group of multiple tasks, with the key characteristic: Each pipeline is executed is a separate pod Multiple pipelines are connected by a directed acyclic graph (DAG) Pipelines are executed in the graph order kubectl apply -f k8s-apps/tekton/pipeline/maven-build-pipeline.yaml","title":"Create a Pipeline"},{"location":"k8s-apps/tekton/tekton-pipeline/#start-a-pipeline-run","text":"The Pipeline is like a class in object oriented programming and abstract description, without specific values. A PipelineRun is the instantiation and execution of a Pipeline with specific values. kubectl create -f k8s-apps/tekton/pipeline/run/spring-petclinic-pipeline-run.yaml","title":"Start a pipeline run"},{"location":"k8s-apps/tekton/tekton-pipeline/#urls","text":"https://tekton.dev/docs/concepts/ https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://developers.redhat.com/blog/2020/02/26/speed-up-maven-builds-in-tekton-pipelines/","title":"URLs"}]}