{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes-Playground Handbook The Kubernetes-Playground is a solution to setup a simple Kubernetes-Cluster with minimal effort, to learn and play with Kubernetes, see GitHub Project layout . # Project root \u251c\u2500\u2500 .github # the github workflow files \u251c\u2500\u2500 bin # project specific binaries, like plantuml.jar; excluded from version control \u251c\u2500\u2500 docs # documentation \u251c\u2500\u2500 kubernetes-setup # Ansible scripts to provision the vagrant box \u251c\u2500\u2500 mkdocs.yml # configuration file for mkdocs \u251c\u2500\u2500 README.md # Readme file for github \u2514\u2500\u2500 Vagrantfile # Configuration of the vagrant box","title":"Home"},{"location":"#kubernetes-playground-handbook","text":"The Kubernetes-Playground is a solution to setup a simple Kubernetes-Cluster with minimal effort, to learn and play with Kubernetes, see GitHub","title":"Kubernetes-Playground Handbook"},{"location":"#project-layout","text":". # Project root \u251c\u2500\u2500 .github # the github workflow files \u251c\u2500\u2500 bin # project specific binaries, like plantuml.jar; excluded from version control \u251c\u2500\u2500 docs # documentation \u251c\u2500\u2500 kubernetes-setup # Ansible scripts to provision the vagrant box \u251c\u2500\u2500 mkdocs.yml # configuration file for mkdocs \u251c\u2500\u2500 README.md # Readme file for github \u2514\u2500\u2500 Vagrantfile # Configuration of the vagrant box","title":"Project layout"},{"location":"documentation-guide/","text":"Documentation Guide Mkdocs and PlantUML is used to generate the Handbook of this project. It is helpful to install mkdocs to generate a searchable html version of this handbook. Mkdocs Commands # Create a new project. mkdocs new [dir-name] # Start the live-reloading docs server. mkdocs serve # Build the documentation site. mkdocs build # Print this help message. mkdocs help PlantUML PlantUML is a tool to create diagrams from plain text files must be placed in the folder docs/diagrams as a suffix for PlantUML files, .plantuml is used this is required for the Makefile, to create image on the local machine and the GitHub Action to build and deploy the GitHub Page of this project The Visual Studio Code plugin, provides code highlighting, code completion and a live preview, ... Example Diagram Makefile The Makefile to create png or svg images from plain text is in the docs folder it is just needed for local development In the case that plantuml is not in the bin folder of this project, it will be automatically downloaded Overview Targets Target Description all Build png and svg images png Build png images svg Build svg images clean Delete all png and svg images URLs https://plantuml.com/ https://marketplace.visualstudio.com/items?itemName=jebbs.plantuml","title":"Documentation"},{"location":"documentation-guide/#documentation-guide","text":"Mkdocs and PlantUML is used to generate the Handbook of this project. It is helpful to install mkdocs to generate a searchable html version of this handbook.","title":"Documentation Guide"},{"location":"documentation-guide/#mkdocs-commands","text":"# Create a new project. mkdocs new [dir-name] # Start the live-reloading docs server. mkdocs serve # Build the documentation site. mkdocs build # Print this help message. mkdocs help","title":"Mkdocs Commands"},{"location":"documentation-guide/#plantuml","text":"PlantUML is a tool to create diagrams from plain text files must be placed in the folder docs/diagrams as a suffix for PlantUML files, .plantuml is used this is required for the Makefile, to create image on the local machine and the GitHub Action to build and deploy the GitHub Page of this project The Visual Studio Code plugin, provides code highlighting, code completion and a live preview, ...","title":"PlantUML"},{"location":"documentation-guide/#example-diagram","text":"","title":"Example Diagram"},{"location":"documentation-guide/#makefile","text":"The Makefile to create png or svg images from plain text is in the docs folder it is just needed for local development In the case that plantuml is not in the bin folder of this project, it will be automatically downloaded","title":"Makefile"},{"location":"documentation-guide/#overview-targets","text":"Target Description all Build png and svg images png Build png images svg Build svg images clean Delete all png and svg images","title":"Overview Targets"},{"location":"documentation-guide/#urls","text":"https://plantuml.com/ https://marketplace.visualstudio.com/items?itemName=jebbs.plantuml","title":"URLs"},{"location":"installation/","text":"Installation Guide The Kubernetes-Playground is build and tested on Ubuntu 20.04. Hence, the described steps are focusing on Ubuntu. It should be possible to use any Operation system as a host. Required Before starting to use this project, make sure that the following software is installed on the system: VirtualBox, https://www.virtualbox.org/wiki/Downloads Vagrant, https://www.vagrantup.com/downloads apt install virtualbox vagrant Optional - Documentation To generate a searchable HTML documentation mkdocs must be installed on the system # Install mkdocs sudo apt install mkdocs # Install pip sudo apt install python3-pip # Make sure latest version is used pip3 install --upgrade mkdocs Plantuml By the first use of he makefile in the docs folder, plantuml is downloaded For some diagrams plantuml depends on graphviz. Hence, it must be installed. # Install graphviz sudo apt install graphviz","title":"Installation"},{"location":"installation/#installation-guide","text":"The Kubernetes-Playground is build and tested on Ubuntu 20.04. Hence, the described steps are focusing on Ubuntu. It should be possible to use any Operation system as a host.","title":"Installation Guide"},{"location":"installation/#required","text":"Before starting to use this project, make sure that the following software is installed on the system: VirtualBox, https://www.virtualbox.org/wiki/Downloads Vagrant, https://www.vagrantup.com/downloads apt install virtualbox vagrant","title":"Required"},{"location":"installation/#optional-documentation","text":"To generate a searchable HTML documentation mkdocs must be installed on the system # Install mkdocs sudo apt install mkdocs # Install pip sudo apt install python3-pip # Make sure latest version is used pip3 install --upgrade mkdocs","title":"Optional - Documentation"},{"location":"installation/#plantuml","text":"By the first use of he makefile in the docs folder, plantuml is downloaded For some diagrams plantuml depends on graphviz. Hence, it must be installed. # Install graphviz sudo apt install graphviz","title":"Plantuml"},{"location":"kubectl-config/","text":"Configuration of kubectl The next bash snippets outlines, the installation and configuration to use kubectl to access microk8s in a vagrant box. Install kubectl # Install kubectl via snap sudo snap install kubectl --classic kubectl configuration # Create the configuration folder in the home path mkdir .kube # get the configuration from the vagrant box # if the config file already exist, it is needed to merge them vagrant ssh -c microk8s.config >> ~/.kube/config # change the server ip to the external ip of the vagrant box or k8s.home vi ~/.kube/config Instead of the external IP Address of the vagrant box it is also possible to use the domain k8s.home . Hence, the local DNS must resolve the domain to the IP of the vagrant box. This can be done by modifying the /etc/hosts file or the configuration of the local DNS resolver like Unbound. Configuration via update script The updateKubeConfig.sh provides the following functionality Copy the configuration from the vagrant box, to the current directory Replace the IP with the domain k8s.home set the environment variable KUBECONFIG to the file from the first step the first dot is important; otherwise the environment variable is not set from the current shell, only this configuration file is usable the configuration applies only temporarily to the open shell session . ./updateKubeConfig.sh bash bash-completion sudo kubectl completion bash >/etc/bash_completion.d/kubectl","title":"Kubectl"},{"location":"kubectl-config/#configuration-of-kubectl","text":"The next bash snippets outlines, the installation and configuration to use kubectl to access microk8s in a vagrant box.","title":"Configuration of kubectl"},{"location":"kubectl-config/#install-kubectl","text":"# Install kubectl via snap sudo snap install kubectl --classic","title":"Install kubectl"},{"location":"kubectl-config/#kubectl-configuration","text":"# Create the configuration folder in the home path mkdir .kube # get the configuration from the vagrant box # if the config file already exist, it is needed to merge them vagrant ssh -c microk8s.config >> ~/.kube/config # change the server ip to the external ip of the vagrant box or k8s.home vi ~/.kube/config Instead of the external IP Address of the vagrant box it is also possible to use the domain k8s.home . Hence, the local DNS must resolve the domain to the IP of the vagrant box. This can be done by modifying the /etc/hosts file or the configuration of the local DNS resolver like Unbound.","title":"kubectl configuration"},{"location":"kubectl-config/#configuration-via-update-script","text":"The updateKubeConfig.sh provides the following functionality Copy the configuration from the vagrant box, to the current directory Replace the IP with the domain k8s.home set the environment variable KUBECONFIG to the file from the first step the first dot is important; otherwise the environment variable is not set from the current shell, only this configuration file is usable the configuration applies only temporarily to the open shell session . ./updateKubeConfig.sh","title":"Configuration via update script"},{"location":"kubectl-config/#bash-bash-completion","text":"sudo kubectl completion bash >/etc/bash_completion.d/kubectl","title":"bash bash-completion"},{"location":"network-configuration/","text":"Network Configuration Sorry, this is an open todo","title":"Network Configuration"},{"location":"network-configuration/#network-configuration","text":"Sorry, this is an open todo","title":"Network Configuration"},{"location":"vagrant/","text":"Vagrant Vagrant is an open-source software to automate the provision of virtual machine. For this playground, it is used to rapidly setup a Ubuntu guest system within VirtualBox. Commands This sections covers the most important commands to use vagrant for more information use vagrant --help or man vagrant # starts and provisions the vagrant environment vagrant up # stops the vagrant environment vagrant halt # login via ssh vagrant ssh # provisions to a running vagrant machine; useful by a change of the ansible scripts vagrant provision # stops and deletes the vagrant machine vagrant destroy Ansible Vagrant offers integration for various code as infrastructure tools, among other for Ansible . In order to avoid a Ansible installation on the host system, the Vagrant ansible_local provisioner is used. Before executing the Ansible playbook, Vagrant installs Ansible on the guest system. As no Ansible installation is required on the host system, a the portability is improved, due to a reduction of dependencies. Ansible Provisioning The ansible project is in the folder *kubernetes-setup Vagrant use the Playbook in the master-playbook.yml file The role defaultSetup , installs some useful linux packages like htop or mc The role microK8s , installation of Kubernetes itself The role k8sDefaultAddons , enabling the microK8 addons DNS, Ingress and Storage by default. This should reduce re-occurring and error prone configuration tasks. By adding the addon name in the file kubernetes-setup/k8sDefaultAddons/tasks/enableK8sDefaultAddons.yml , more addons can be enabled by default. URLs https://en.wikipedia.org/wiki/Vagrant_(software)","title":"Vagrant"},{"location":"vagrant/#vagrant","text":"Vagrant is an open-source software to automate the provision of virtual machine. For this playground, it is used to rapidly setup a Ubuntu guest system within VirtualBox.","title":"Vagrant"},{"location":"vagrant/#commands","text":"This sections covers the most important commands to use vagrant for more information use vagrant --help or man vagrant # starts and provisions the vagrant environment vagrant up # stops the vagrant environment vagrant halt # login via ssh vagrant ssh # provisions to a running vagrant machine; useful by a change of the ansible scripts vagrant provision # stops and deletes the vagrant machine vagrant destroy","title":"Commands"},{"location":"vagrant/#ansible","text":"Vagrant offers integration for various code as infrastructure tools, among other for Ansible . In order to avoid a Ansible installation on the host system, the Vagrant ansible_local provisioner is used. Before executing the Ansible playbook, Vagrant installs Ansible on the guest system. As no Ansible installation is required on the host system, a the portability is improved, due to a reduction of dependencies.","title":"Ansible"},{"location":"vagrant/#ansible-provisioning","text":"The ansible project is in the folder *kubernetes-setup Vagrant use the Playbook in the master-playbook.yml file The role defaultSetup , installs some useful linux packages like htop or mc The role microK8s , installation of Kubernetes itself The role k8sDefaultAddons , enabling the microK8 addons DNS, Ingress and Storage by default. This should reduce re-occurring and error prone configuration tasks. By adding the addon name in the file kubernetes-setup/k8sDefaultAddons/tasks/enableK8sDefaultAddons.yml , more addons can be enabled by default.","title":"Ansible Provisioning"},{"location":"vagrant/#urls","text":"https://en.wikipedia.org/wiki/Vagrant_(software)","title":"URLs"},{"location":"k8s-apps/jenkins/","text":"Jenkins Operator This section will guide you through the process of installing Jenkins in the Kubernetes cluster. Enable DNS During the installation Jenkins will fetch updates, therefore DNS is needed. # Enable DNS vagrant ssh -c \"microk8s.enable dns\" Install Operator Configure Custom Resource Definition Create the custom resource definition for the jenkins operator # Apply the custom resource definition kubectl apply -f https://raw.githubusercontent.com/jenkinsci/kubernetes-operator/master/deploy/crds/jenkins_v1alpha2_jenkins_crd.yaml # Get all custom resource definition - there should be two custom resource definition: # - jenkins.jenkins.io # - jenkinsimages.jenkins.io kubectl get customresourcedefinitions.apiextensions.k8s.io # Delete the custom resource definition kubectl delete customresourcedefinitions.apiextensions.k8s.io jenkins.jenkins.io kubectl delete customresourcedefinitions.apiextensions.k8s.io jenkinsimages.jenkins.io Deploy Operator - using yaml Create the Operator in the default namespace kubectl apply -f https://raw.githubusercontent.com/jenkinsci/kubernetes-operator/master/deploy/all-in-one-v1alpha2.yaml In the default namespace should be a pod running with the name jenkins-operator-* . This can be checked with: kubectl get pods --all-namespaces Deploy Jenkins kubectl apply -f k8s-apps/jenkins/jenkins_instance.yaml After a while a jenkins pod should be running watch kubectl get pods In the case that the jenkins pod is in a restarting loop, check if DNS is enabled. # Connect to the vagrant box vagrant ssh # Check the status - dns should be listed by the enabled addons microk8s.status # or as a one liner vagrant ssh -c \"microk8s.status\" Credentials The postfix *- my-jenkins , must be same as in the jenkins_instance.yaml in the section metadata.name # The user Name of the Jenkins instance echo \"User:\"; kubectl get secret jenkins-operator-credentials-my-jenkins -o 'jsonpath={.data.user}' | base64 -d | xargs echo echo \"Password:\"; kubectl get secret jenkins-operator-credentials-my-jenkins -o 'jsonpath={.data.password}' | base64 -d | xargs echo Access the Jenkins By default Kubernetes is not routing traffic to any pods. It is possible to route your local traffic to Kubernetes by creating a port-forward # Create a port-forwarding from local host to the kubectl port-forward jenkins-my-jenkins 8080:8080 After executing this command, it is possible to open the URL http://127.0.0.1:8080 and login to Jenkins. URLs https://jenkinsci.github.io/kubernetes-operator/docs/ https://github.com/jenkinsci/kubernetes-operator The files (such as custom resource definition), are in the deploy folder","title":"Jenkins"},{"location":"k8s-apps/jenkins/#jenkins-operator","text":"This section will guide you through the process of installing Jenkins in the Kubernetes cluster.","title":"Jenkins Operator"},{"location":"k8s-apps/jenkins/#enable-dns","text":"During the installation Jenkins will fetch updates, therefore DNS is needed. # Enable DNS vagrant ssh -c \"microk8s.enable dns\"","title":"Enable DNS"},{"location":"k8s-apps/jenkins/#install-operator","text":"","title":"Install Operator"},{"location":"k8s-apps/jenkins/#configure-custom-resource-definition","text":"Create the custom resource definition for the jenkins operator # Apply the custom resource definition kubectl apply -f https://raw.githubusercontent.com/jenkinsci/kubernetes-operator/master/deploy/crds/jenkins_v1alpha2_jenkins_crd.yaml # Get all custom resource definition - there should be two custom resource definition: # - jenkins.jenkins.io # - jenkinsimages.jenkins.io kubectl get customresourcedefinitions.apiextensions.k8s.io # Delete the custom resource definition kubectl delete customresourcedefinitions.apiextensions.k8s.io jenkins.jenkins.io kubectl delete customresourcedefinitions.apiextensions.k8s.io jenkinsimages.jenkins.io","title":"Configure Custom Resource Definition"},{"location":"k8s-apps/jenkins/#deploy-operator-using-yaml","text":"Create the Operator in the default namespace kubectl apply -f https://raw.githubusercontent.com/jenkinsci/kubernetes-operator/master/deploy/all-in-one-v1alpha2.yaml In the default namespace should be a pod running with the name jenkins-operator-* . This can be checked with: kubectl get pods --all-namespaces","title":"Deploy Operator - using yaml"},{"location":"k8s-apps/jenkins/#deploy-jenkins","text":"kubectl apply -f k8s-apps/jenkins/jenkins_instance.yaml After a while a jenkins pod should be running watch kubectl get pods In the case that the jenkins pod is in a restarting loop, check if DNS is enabled. # Connect to the vagrant box vagrant ssh # Check the status - dns should be listed by the enabled addons microk8s.status # or as a one liner vagrant ssh -c \"microk8s.status\"","title":"Deploy Jenkins"},{"location":"k8s-apps/jenkins/#credentials","text":"The postfix *- my-jenkins , must be same as in the jenkins_instance.yaml in the section metadata.name # The user Name of the Jenkins instance echo \"User:\"; kubectl get secret jenkins-operator-credentials-my-jenkins -o 'jsonpath={.data.user}' | base64 -d | xargs echo echo \"Password:\"; kubectl get secret jenkins-operator-credentials-my-jenkins -o 'jsonpath={.data.password}' | base64 -d | xargs echo","title":"Credentials"},{"location":"k8s-apps/jenkins/#access-the-jenkins","text":"By default Kubernetes is not routing traffic to any pods. It is possible to route your local traffic to Kubernetes by creating a port-forward # Create a port-forwarding from local host to the kubectl port-forward jenkins-my-jenkins 8080:8080 After executing this command, it is possible to open the URL http://127.0.0.1:8080 and login to Jenkins.","title":"Access the Jenkins"},{"location":"k8s-apps/jenkins/#urls","text":"https://jenkinsci.github.io/kubernetes-operator/docs/ https://github.com/jenkinsci/kubernetes-operator The files (such as custom resource definition), are in the deploy folder","title":"URLs"},{"location":"k8s-apps/nginx/","text":"Ngnix Deployment Diagram Deploy Ngnix Deployment In the first step three Nginx instances are deployed to the Kubernetes Cluster . All three instances providing the same content, a simple \"Hello World\" # Creates the deployment and rollout three nginx instances kubectl apply -f k8s-apps/nginx/nginx-deployment.yaml Next the nginx-service is deployed, this is a kubernetes resource to expose the pods to the internal network of the cluster . Service # Create a service which will route the traffic to one of the three nginx pods kubectl apply -f k8s-apps/nginx/nginx-service.yaml The pods are dynamically deployments, which means that pods can be created and destroyed at anytime. Consequently, the network configuration such as IP Addresses will change. Besides, all three pods are serving the same content and it is just relevant which pod will handle the request. A Kubernetes Service solve this by giving as a stable DNS name and ensuring that the request it routed to one of the three pods. Testing the Service Services are just accessible within the Cluster , thus to test the configuration it is needed to create a Port-Forward from the client to the Kubernetes Cluster . # Create a port-forwarding from local host to the service kubectl port-forward service/my-ngnix-service 8080:8080 Opening http://192.168.60.10:8080 in a browser should show us the default Nginx start page. Ingress The goal of the last step is to expose the Nginx to the outside. Network traffic which is entering a network is called ingress , traffic leaving a network is called egress . A Kubernetes Ingress consists of an Ingress Controller and an Ingress Resources . The controller is in a nutshell a http proxy, which will forward the traffic to the service/pod. In contrast is the resource the configuration of the controller behavior. It is possible to use the same controller for several resources. # Enable a Ingress controller vagrant ssh -c \"microk8s.enable ingress\" # Create a ingress kubectl apply -f k8s-apps/nginx/nginx-ingress.yaml The nginx is now reachable from outside the cluster, to test it open http://192.168.60.10/path-of-my-ingress in a Browser. How related resources are tied The pod deployment resource file nginx-deployment.yaml , specifics labels. These labels are useful to select a set of pods, e.g. for searching pods or linking resources together. In the service resource file nginx-service.yaml the selector directive is used to select all pods with a specific set of labels. In this example the label app: nginx is used. The name directive is used in the ingress resource nginx-service.yaml , to connect the ingress with the service. Routing Network traffic The nginx-ingress accept http request on port 80 from the client. All traffic on port 80 is forwarded from the nginx-ingress to the nginx-service on Port 8080 The request which the nginx-service received on port 8080 is forwarded to exactly one nginx-pod on port 80 URLs https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ https://kubernetes.io/docs/concepts/services-networking/service/ https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/ https://kubernetes.io/docs/concepts/services-networking/ingress/","title":"Nginx"},{"location":"k8s-apps/nginx/#ngnix","text":"","title":"Ngnix"},{"location":"k8s-apps/nginx/#deployment-diagram","text":"","title":"Deployment Diagram"},{"location":"k8s-apps/nginx/#deploy-ngnix","text":"","title":"Deploy Ngnix"},{"location":"k8s-apps/nginx/#deployment","text":"In the first step three Nginx instances are deployed to the Kubernetes Cluster . All three instances providing the same content, a simple \"Hello World\" # Creates the deployment and rollout three nginx instances kubectl apply -f k8s-apps/nginx/nginx-deployment.yaml Next the nginx-service is deployed, this is a kubernetes resource to expose the pods to the internal network of the cluster .","title":"Deployment"},{"location":"k8s-apps/nginx/#service","text":"# Create a service which will route the traffic to one of the three nginx pods kubectl apply -f k8s-apps/nginx/nginx-service.yaml The pods are dynamically deployments, which means that pods can be created and destroyed at anytime. Consequently, the network configuration such as IP Addresses will change. Besides, all three pods are serving the same content and it is just relevant which pod will handle the request. A Kubernetes Service solve this by giving as a stable DNS name and ensuring that the request it routed to one of the three pods.","title":"Service"},{"location":"k8s-apps/nginx/#testing-the-service","text":"Services are just accessible within the Cluster , thus to test the configuration it is needed to create a Port-Forward from the client to the Kubernetes Cluster . # Create a port-forwarding from local host to the service kubectl port-forward service/my-ngnix-service 8080:8080 Opening http://192.168.60.10:8080 in a browser should show us the default Nginx start page.","title":"Testing the Service"},{"location":"k8s-apps/nginx/#ingress","text":"The goal of the last step is to expose the Nginx to the outside. Network traffic which is entering a network is called ingress , traffic leaving a network is called egress . A Kubernetes Ingress consists of an Ingress Controller and an Ingress Resources . The controller is in a nutshell a http proxy, which will forward the traffic to the service/pod. In contrast is the resource the configuration of the controller behavior. It is possible to use the same controller for several resources. # Enable a Ingress controller vagrant ssh -c \"microk8s.enable ingress\" # Create a ingress kubectl apply -f k8s-apps/nginx/nginx-ingress.yaml The nginx is now reachable from outside the cluster, to test it open http://192.168.60.10/path-of-my-ingress in a Browser.","title":"Ingress"},{"location":"k8s-apps/nginx/#how-related-resources-are-tied","text":"The pod deployment resource file nginx-deployment.yaml , specifics labels. These labels are useful to select a set of pods, e.g. for searching pods or linking resources together. In the service resource file nginx-service.yaml the selector directive is used to select all pods with a specific set of labels. In this example the label app: nginx is used. The name directive is used in the ingress resource nginx-service.yaml , to connect the ingress with the service.","title":"How related resources are tied"},{"location":"k8s-apps/nginx/#routing-network-traffic","text":"The nginx-ingress accept http request on port 80 from the client. All traffic on port 80 is forwarded from the nginx-ingress to the nginx-service on Port 8080 The request which the nginx-service received on port 8080 is forwarded to exactly one nginx-pod on port 80","title":"Routing Network traffic"},{"location":"k8s-apps/nginx/#urls","text":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ https://kubernetes.io/docs/concepts/services-networking/service/ https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/ https://kubernetes.io/docs/concepts/services-networking/ingress/","title":"URLs"},{"location":"k8s-apps/tekton/tekton-installation/","text":"Tekton Installation Tekton an cloud native open-source framework for creating CI/CD systems. The next steps are describing the installation process on the Kubernetes-playground. Install Pipelines Install the core component Tekton Pipelines kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml After a moment Tekton Pipline should run in his own namespace. # Check the status of the tekton pods kubectl get pods --namespace tekton-pipelines # or use watch for auto refresh watch kubectl get pods --namespace tekton-pipelines Tekton CLI Install tkn the Tekton command line interface. sudo apt update sudo apt install -y gnupg sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3EFE0E0A2F2F60AA echo \"deb http://ppa.launchpad.net/tektoncd/cli/ubuntu eoan main\"|sudo tee /etc/apt/sources.list.d/tektoncd-ubuntu-cli.list sudo apt update && sudo apt install -y tektoncd-cli Tekton Dashboard kubectl apply --filename https://github.com/tektoncd/dashboard/releases/latest/download/tekton-dashboard-release.yaml Access via port-forward After the setup of a port-forward to the tekton-dashboard service, it is possible to access the Dashboard in your Browser with the URL http://127.0.0.1:9097 kubectl --namespace tekton-pipelines port-forward svc/tekton-dashboard 9097:9097 Ingress Next the Tekton Dashboard is exposed to the outside. Therefore, a ingress configuration is created. The concept of ingress controllers are explained in the Nginx chapter. # Enable a Ingress controller vagrant ssh -c \"microk8s.enable ingress\" # Create a ingress kubectl apply -f k8s-apps/tekton/tekton-dashboard-ingress.yaml The tekton-dashboard is now reachable from outside of the cluster, via the URL http://tekton.k8s.home . Instead of the external cluster IP Address, a sub-domain is used to access the tekton-dashboard . This makes the ingress configuration easier, when a web-application loads resources by their absolute URL; otherwise the URL rewriting in the ingress configuration gets challenging. For details, see the network configuration chapter. Hello Tekton Task In the next steps a simple hello world task is created, executed and the result is checked. The taskrun is visible on the shell and the dashboard. # Create the hello task in the default namespace kubectl apply -f k8s-apps/tekton/hello-task.yaml # Dry run from the shell tkn task start hello --dry-run # Start the command form the shell tkn task start hello # View the logs of the last taskrun tkn taskrun logs --last -f URLs https://tekton.dev https://tekton.dev/docs/getting-started/#installation","title":"Installation"},{"location":"k8s-apps/tekton/tekton-installation/#tekton-installation","text":"Tekton an cloud native open-source framework for creating CI/CD systems. The next steps are describing the installation process on the Kubernetes-playground.","title":"Tekton Installation"},{"location":"k8s-apps/tekton/tekton-installation/#install-pipelines","text":"Install the core component Tekton Pipelines kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml After a moment Tekton Pipline should run in his own namespace. # Check the status of the tekton pods kubectl get pods --namespace tekton-pipelines # or use watch for auto refresh watch kubectl get pods --namespace tekton-pipelines","title":"Install Pipelines"},{"location":"k8s-apps/tekton/tekton-installation/#tekton-cli","text":"Install tkn the Tekton command line interface. sudo apt update sudo apt install -y gnupg sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3EFE0E0A2F2F60AA echo \"deb http://ppa.launchpad.net/tektoncd/cli/ubuntu eoan main\"|sudo tee /etc/apt/sources.list.d/tektoncd-ubuntu-cli.list sudo apt update && sudo apt install -y tektoncd-cli","title":"Tekton CLI"},{"location":"k8s-apps/tekton/tekton-installation/#tekton-dashboard","text":"kubectl apply --filename https://github.com/tektoncd/dashboard/releases/latest/download/tekton-dashboard-release.yaml","title":"Tekton Dashboard"},{"location":"k8s-apps/tekton/tekton-installation/#access-via-port-forward","text":"After the setup of a port-forward to the tekton-dashboard service, it is possible to access the Dashboard in your Browser with the URL http://127.0.0.1:9097 kubectl --namespace tekton-pipelines port-forward svc/tekton-dashboard 9097:9097","title":"Access via port-forward"},{"location":"k8s-apps/tekton/tekton-installation/#ingress","text":"Next the Tekton Dashboard is exposed to the outside. Therefore, a ingress configuration is created. The concept of ingress controllers are explained in the Nginx chapter. # Enable a Ingress controller vagrant ssh -c \"microk8s.enable ingress\" # Create a ingress kubectl apply -f k8s-apps/tekton/tekton-dashboard-ingress.yaml The tekton-dashboard is now reachable from outside of the cluster, via the URL http://tekton.k8s.home . Instead of the external cluster IP Address, a sub-domain is used to access the tekton-dashboard . This makes the ingress configuration easier, when a web-application loads resources by their absolute URL; otherwise the URL rewriting in the ingress configuration gets challenging. For details, see the network configuration chapter.","title":"Ingress"},{"location":"k8s-apps/tekton/tekton-installation/#hello-tekton-task","text":"In the next steps a simple hello world task is created, executed and the result is checked. The taskrun is visible on the shell and the dashboard. # Create the hello task in the default namespace kubectl apply -f k8s-apps/tekton/hello-task.yaml # Dry run from the shell tkn task start hello --dry-run # Start the command form the shell tkn task start hello # View the logs of the last taskrun tkn taskrun logs --last -f","title":"Hello Tekton Task"},{"location":"k8s-apps/tekton/tekton-installation/#urls","text":"https://tekton.dev https://tekton.dev/docs/getting-started/#installation","title":"URLs"},{"location":"k8s-apps/tekton/tekton-pipeline/","text":"Tekton Pipeline In this next steps a Tekton Task and Pipeline is configured to build the Spring PetClinic Sample Application . As the Spring PetClinic is a Maven based project, a persistent volume claim is configured, to cache the dependencies. Prepare the cluster # Enable storage for a persistence volume claim, to cache the maven dependencies vagrant ssh -c \"microk8s.enable storage\" # Enable DNS, to resolve domain names during the build vagrant ssh -c \"microk8s.enable dns\" Create a PersistentVolumeClaim First, the persistence volume claim is created, to cache the maven dependencies. From the second build, a download of the dependencies can avoid and the build will gain speed, as . kubectl apply -f k8s-apps/tekton/pipeline/maven-repo-pvc.yaml Create a Task A task is a series of steps , which are executed in the same kubernetes pod. Each step runs in separate container in the same pod. This maven task is independent form a specific maven project and can be reused across multiple maven project. kubectl apply -f k8s-apps/tekton/pipeline/maven-task.yaml Create a Pipeline Pipelines are a group of multiple tasks, with the key characteristic: Each pipeline is executed is a separate pod Multiple pipelines are connected by a directed acyclic graph (DAG) Pipelines are executed in the graph order kubectl apply -f k8s-apps/tekton/pipeline/maven-build-pipeline.yaml Start a pipeline run The Pipeline is like a class in object oriented programming and abstract description, without specific values. A PipelineRun is the instantiation and execution of a Pipeline with specific values. kubectl create -f k8s-apps/tekton/pipeline/run/spring-petclinic-pipeline-run.yaml URLs https://tekton.dev/docs/concepts/ https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://developers.redhat.com/blog/2020/02/26/speed-up-maven-builds-in-tekton-pipelines/","title":"Pipeline"},{"location":"k8s-apps/tekton/tekton-pipeline/#tekton-pipeline","text":"In this next steps a Tekton Task and Pipeline is configured to build the Spring PetClinic Sample Application . As the Spring PetClinic is a Maven based project, a persistent volume claim is configured, to cache the dependencies.","title":"Tekton Pipeline"},{"location":"k8s-apps/tekton/tekton-pipeline/#prepare-the-cluster","text":"# Enable storage for a persistence volume claim, to cache the maven dependencies vagrant ssh -c \"microk8s.enable storage\" # Enable DNS, to resolve domain names during the build vagrant ssh -c \"microk8s.enable dns\"","title":"Prepare the cluster"},{"location":"k8s-apps/tekton/tekton-pipeline/#create-a-persistentvolumeclaim","text":"First, the persistence volume claim is created, to cache the maven dependencies. From the second build, a download of the dependencies can avoid and the build will gain speed, as . kubectl apply -f k8s-apps/tekton/pipeline/maven-repo-pvc.yaml","title":"Create a PersistentVolumeClaim"},{"location":"k8s-apps/tekton/tekton-pipeline/#create-a-task","text":"A task is a series of steps , which are executed in the same kubernetes pod. Each step runs in separate container in the same pod. This maven task is independent form a specific maven project and can be reused across multiple maven project. kubectl apply -f k8s-apps/tekton/pipeline/maven-task.yaml","title":"Create a Task"},{"location":"k8s-apps/tekton/tekton-pipeline/#create-a-pipeline","text":"Pipelines are a group of multiple tasks, with the key characteristic: Each pipeline is executed is a separate pod Multiple pipelines are connected by a directed acyclic graph (DAG) Pipelines are executed in the graph order kubectl apply -f k8s-apps/tekton/pipeline/maven-build-pipeline.yaml","title":"Create a Pipeline"},{"location":"k8s-apps/tekton/tekton-pipeline/#start-a-pipeline-run","text":"The Pipeline is like a class in object oriented programming and abstract description, without specific values. A PipelineRun is the instantiation and execution of a Pipeline with specific values. kubectl create -f k8s-apps/tekton/pipeline/run/spring-petclinic-pipeline-run.yaml","title":"Start a pipeline run"},{"location":"k8s-apps/tekton/tekton-pipeline/#urls","text":"https://tekton.dev/docs/concepts/ https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://developers.redhat.com/blog/2020/02/26/speed-up-maven-builds-in-tekton-pipelines/","title":"URLs"}]}